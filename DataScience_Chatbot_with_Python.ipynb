{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilirjanahyseni/data-science-chatbot/blob/main/DataScience_Chatbot_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program is designed to function as a chatbot with the ability to scrape data from the Data Science Wikipedia page and answer user questions. It combines traditional text processing methods with advanced machine learning models for question answering. Here's a breakdown of its main components:\n",
        "\n",
        "1. Library Imports: The script begins by importing necessary Python libraries for HTTP requests, HTML parsing, natural language processing, vectorization, and machine learning.\n",
        "\n",
        "2. NLTK Data Download: It ensures that essential NLTK data packages (punkt and wordnet) are available, which are crucial for text tokenization and lemmatization.\n",
        "\n",
        "3. Question-Answering Pipeline: The script initializes a question-answering pipeline using Hugging Face's Transformers library, which employs a pre-trained model capable of understanding and answering natural language questions.\n",
        "\n",
        "4. Web Scraping: It fetches and parses the content from the Data Science Wikipedia page using requests and BeautifulSoup, storing the text in a lowercased format for further processing.\n",
        "\n",
        "5. Text Preprocessing: The script sets up functions for tokenizing and normalizing text, including lemmatization and punctuation removal, to prepare the data for analysis.\n",
        "\n",
        "6. Response Generation: The response function uses TF-IDF vectorization and cosine similarity to generate responses based on the user's input, leveraging the scraped Wikipedia content.\n",
        "\n",
        "7. Enhanced Question Answering: For specific questions (detected by keywords like \"what is\" or \"explain\"), the script uses the pre-trained model to generate more accurate and context-aware responses.\n",
        "\n",
        "8. Interaction Loop: Finally, the script enters a loop where it interacts with the user, processing input, generating responses using either the traditional method or the pre-trained model, and continuing the conversation until the user opts to exit.\n"
      ],
      "metadata": {
        "id": "wqYuPsby3pZf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4BmyocFbb0MJ"
      },
      "outputs": [],
      "source": [
        "import requests # Used to make HTTP requests to fetch the Wikipedia page\n",
        "from bs4 import BeautifulSoup # Parses the HTML content of the Wikipedia page to extract text\n",
        "import nltk # The Natural Language Toolkit, used for text processing like tokenization and lemmatization\n",
        "import numpy as np # Numerical operations\n",
        "import random # Generating random choices\n",
        "import string # Provides a list of punctuation characters for text cleaning\n",
        "\n",
        "# Transform text into a vectorized form and calculating similarity between text segments\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Provides access to the question-answering pipeline and pre-trained models\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading NLTK packages\n",
        "nltk.download('punkt', quiet=True) # for sentence tokenization\n",
        "nltk.download('wordnet', quiet=True) #  for lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uC49xTu6SA2",
        "outputId": "3e879e99-44b9-471b-a44b-dda5ec8e34fb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question-Answering Pipeline Initialization**:\n",
        "\n",
        "- Initializes a pipeline for question-answering tasks using a pre-trained model.\n",
        "- The default model is associated with the question-answering pipeline from Hugging Face's Transformers library. When you call pipeline('question-answering') without specifying a model, it defaults to a pre-trained model (distilbert-base-cased-distilled-squad.) that's optimized for the question-answering task."
      ],
      "metadata": {
        "id": "jkoI03ESOrYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the question-answering pipeline with a pre-trained model\n",
        "qa_pipeline = pipeline('question-answering')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6OU3eUCRlX",
        "outputId": "aba78658-accf-4c39-8abc-3f3454d6f969"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To specify a different model for the task, you can do so by providing the model's identifier when you initialize the pipeline. For example:\n",
        "\n",
        "qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "This would initialize the pipeline with a different BERT variant that's also fine-tuned on the SQuAD dataset.\n"
      ],
      "metadata": {
        "id": "RLtsDU7PPuZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraping:**\n",
        "\n",
        "- The script fetches the Data Science Wikipedia page using requests.\n",
        "- BeautifulSoup is used to parse the HTML and extract all paragraph text (p tags), which is then converted to lowercase."
      ],
      "metadata": {
        "id": "2xVFipEHNbmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Web scraping to fetch the content\n",
        "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "text = \" \".join([p.text for p in soup.find_all('p')]).lower()\n",
        "sent_tokens = nltk.sent_tokenize(text)"
      ],
      "metadata": {
        "id": "2PawJs3XDxc0"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing Functions**:\n",
        "\n",
        "- Lemmatization: The script initializes a WordNetLemmatizer to convert words to their base form.\n",
        "- Tokenization and Cleaning: Functions are defined to tokenize text and remove punctuation, preparing it for vectorization."
      ],
      "metadata": {
        "id": "a66ZLg4oN2eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing functions\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "AtqGz6zfD2ku"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Response Generation (Traditional Method)**:\n",
        "\n",
        "- The response function uses TF-IDF to vectorize the text and calculates cosine similarity to find the most similar sentence in the scraped content to the user's input.\n",
        "- If the similarity is too low (indicating no relevant information was found), it returns an apology; otherwise, it returns the most similar sentence."
      ],
      "metadata": {
        "id": "nc-lLw3wN-na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Response generation function\n",
        "def response(user_response):\n",
        "    robo_response = ''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if req_tfidf == 0:\n",
        "        robo_response += \"I am sorry! I don't understand you.\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response += sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "metadata": {
        "id": "mZKXOfHGD6T5"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enhanced Question Answering**:\n",
        "\n",
        "- For queries that are identified as questions (containing phrases like \"what is\" or \"explain\"), the script uses the question-answering pipeline.\n",
        "- get_answer_from_model sends the user's query and the scraped text as context to the pipeline, which then returns a specific answer."
      ],
      "metadata": {
        "id": "qQCgGFzfOJQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhancing the chatbot's ability to answer specific questions\n",
        "def get_answer_from_model(question, context):\n",
        "    return qa_pipeline(question=question, context=context)['answer']"
      ],
      "metadata": {
        "id": "AFXYypMnD_CF"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Interaction Loop**:\n",
        "\n",
        "- The chatbot introduces itself and invites the user to ask questions.\n",
        "- In a loop, the chatbot takes user input, checks if the user wants to exit, and then decides whether to use the traditional response generation method or the enhanced question-answering pipeline based on the type of query.\n",
        "- The chatbot responds with either the generated answer or a farewell message, continuing until the user exits."
      ],
      "metadata": {
        "id": "_MHuNhUaOYP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main interaction loop\n",
        "flag = True\n",
        "print(\"BOT: My name is DataBot. I can answer your questions about data science. If you want to exit, type Bye!\")\n",
        "while flag:\n",
        "    user_response = input(\"You: \")\n",
        "    user_response = user_response.lower()\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thanks' or user_response == 'thank you':\n",
        "            flag = False\n",
        "            print(\"BOT: You're welcome!\")\n",
        "        else:\n",
        "            if 'what is' in user_response or 'explain' in user_response:\n",
        "                print(\"BOT: \", end=\"\")\n",
        "                print(get_answer_from_model(user_response, context=text))\n",
        "            else:\n",
        "                print(\"BOT: \", end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"BOT: Goodbye!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KphG5kiFCnM5",
        "outputId": "ab84beda-624d-4e26-f96e-d66c2f839335"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: My name is DataBot. I can answer your questions about data science. If you want to exit, type Bye!\n",
            "You: What is data science?\n",
            "BOT: the sexiest job of the 21st century\n",
            "You: wow\n",
            "BOT: I am sorry! I don't understand you.\n",
            "You: What is data science ?\n",
            "BOT: the sexiest job of the 21st century\n",
            "You: bye\n",
            "BOT: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5yXpRQMXopU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}